machine: ks
# ---------------
model:
  dit:
    dim: 2048
    n_layers: 32
    n_heads: 32
    n_kv_heads: 32
    latent_embedding_size: 3584

  diffhead:
    hidden_size: 3584
    depth: 2
    x_dim: 8
    z_dim: 3584
  
  num_hat: 4

  tune_backbone: false
# --------------------
feature_down_projector:
  clip_feature_dim: 4096
  feature_dim_output: 8
  ckpt: "/data/phd/jinjiachun/experiment/mmdit/0817_sd3_256/mmdit-mmdit-95000"
# --------------------
train:
  root: null
  resume_path: null
  global_step: 0

  exp_name: &exp_name clip_1024
  wandb_proj: *exp_name
  output_dir: 0825_metaquery_lumina_dit
  logging_dir: logs
  mixed_precision: bf16
  gradient_accumulation_steps: 1
  report_to: wandb

  lr: 1e-4
  num_iter: 500000
  save_every: 5000
# --------------------
data:
  wds_path: [/data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Long-Caption, /data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Short-Caption, /data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-JourneyDB]
  num_train_examples: 35000000
  img_size: 448
  buffer_size: 10000
  batch_size: 5
  num_workers: 8
  cfg_drop_rate: 0.1
  num_img_token: 256
  max_seq_length: 512