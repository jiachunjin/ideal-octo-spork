machine: "ks" # [g4, ks]

vae_aligner:
  pretrained_path: "/data/phd/jinjiachun/experiment/vae_aligner/0714_sd3_vae_aligner_hybrid/vae_aligner-vae_aligner-215k"
  hidden_size: 1024
  depth: 16
  num_heads: 16
  grid_size: 24

  siglip_feature_dim: 1024
  siglip_feature_dim_down: 16

diffhead:
  hidden_size: 1024
  depth: 2
  x_dim: 16
  z_dim: 2048

tune_backbone: false

num_new_layers: 12

train:
  root: null
  diffhead_resume_path: "/data/phd/jinjiachun/experiment/query_dit/0731_hat_training/diff_head-query_dit-25000"
  siglip16_aligner_resume_path: "/data/phd/jinjiachun/experiment/query_dit/0731_hat_training/siglip16_aligner-query_dit-25000"
  backbone_resume_path: "/data/phd/jinjiachun/experiment/query_dit/0731_hat_training/janus-backbone-query_dit-25000"
  global_step: 25000

  ar_backbone: "janus1b" # [janus1b, janus7b]

  exp_name: &exp_name "query_dit"
  wandb_proj: *exp_name
  output_dir: "0731_hat_training"
  logging_dir: "logs"
  mixed_precision: "bf16"
  gradient_accumulation_steps: 1
  report_to: "wandb"

  cfg_drop_rate: 0.1

  lr: 5e-6
  num_iter: 100000
  save_every: 5000


data:
  name: "hybrid"
  understanding:
    img_path: "/data/phd/jinjiachun/dataset/llava_mix665k"
    ann_path: "/data/phd/jinjiachun/dataset/liuhaotian/LLaVA-Instruct-150K/llava_v1_5_mix665k.json"
    batch_size: 5
    num_workers: 8
  generation:
    wds_path: ["/data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Long-Caption", "/data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Short-Caption", "/data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-JourneyDB",]
    img_size: 384
    buffer_size: 80000
    batch_size: 25
    num_workers: 8
  max_seq_length: 768
  num_img_token: 576